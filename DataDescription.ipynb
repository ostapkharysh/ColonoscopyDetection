{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "### Here you could find the data and it's description that is used for our task of Colonoscopy relevantness detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Codebook\n",
    "* quality: a measure of the quality of the recorded video.\n",
    "* bits: number of bits used to encode that block in the video stream.\n",
    "* intra_parts: number sub-blocks inside this block that are not encoded by making use of\n",
    "information in other frames.\n",
    "* skip_parts: number sub-blocks inside this block that are straight-forward copied from another\n",
    "frame.\n",
    "* inter_16x16_parts: number of sub-blocks inside this block making use of information in other\n",
    "frames and whose size is 16x16 pixels.\n",
    "* inter_4x4_parts: number of sub-blocks inside this block making use of information in other\n",
    "frames and whose size is 4x4 pixels.\n",
    "* inter_other_parts: number of sub-blocks inside this block making use of information in other\n",
    "frames and whose size is different from 16x16 and 4x4 pixels.\n",
    "* non_zero_pixels: number of pixels different from 0 after encoding the block.\n",
    "* frame_width: the width of the video frame in pixels.\n",
    "* frame_height: the height of the video frame in pixels.\n",
    "* movement_level: a measure of the level of movement of this frame with respect the previous\n",
    "one.\n",
    "* mean: mean of the pixels of the encoded block.\n",
    "* sub_mean_1: mean of the pixels contained in the first 32x32 sub-bock of the current block.\n",
    "* sub_mean_2: mean of the pixels contained in the second 32x32 sub-bock of the current block.\n",
    "* sub_mean_3: mean of the pixels contained in the third 32x32 sub-bock of the current block.\n",
    "* sub_mean_4: mean of the pixels contained in the fourth 32x32 sub-bock of the current block.\n",
    "* var_sub_blocks: variance of the four previous values.\n",
    "* sobel_h: mean of the pixels of the encoded block after applying the Sobel operator in\n",
    "horizontal direction.\n",
    "* sobel_v: mean of the pixels of the encoded block after applying the Sobel operator in vertical\n",
    "direction.\n",
    "* variance: variance of the pixels of the encoded block.\n",
    "* block_movement_h: a measure of the movement of the current block in the horizontal\n",
    "direction.\n",
    "* block_movement_v: a measure of the movement of the current block in the vertical direction.\n",
    "* var_movement_h: a measure of the variance of the movements inside the current block in the\n",
    "horizontal direction.\n",
    "* var_movement_v: a measure of the variance of the movements inside the current block in the\n",
    "vertical direction.\n",
    "* cost_1: a measure of the cost of encoding this block without partitioning it.\n",
    "* cost_2: a measure of the cost of encoding this block without partitioning it and without\n",
    "considering any movement in it.\n",
    "* relevant: the target variable that indicates whether the current block is relevant (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quality,bits,intra_parts,skip_parts,inter_16x16_parts,inter_4x4_parts,inter_other_parts,non_zero_pixels,frame_width,frame_height,movement_level,mean,sub_mean_1,sub_mean_2,sub_mean_3,sub_mean_4,var_sub_blocks,sobel_h,sobel_v,variance,block_movement_h,block_movement_v,var_movement_h,var_movement_v,cost_1,cost_2,relevant    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#There are several variables with missing data\n",
    "df = pd.read_csv('data.csv', sep=';')\n",
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16000, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of missing data is not significant, so we drop rows with misssing data\n",
    "df  = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "quality,bits,intra_parts,skip_parts,inter_16x16_parts,inter_4x4_parts,inter_other_parts,non_zero_pixels,frame_width,frame_height,movement_level,mean,sub_mean_1,sub_mean_2,sub_mean_3,sub_mean_4,var_sub_blocks,sobel_h,sobel_v,variance,block_movement_h,block_movement_v,var_movement_h,var_movement_v,cost_1,cost_2,relevant    16000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we explored the number of unique values for each variable and decide which variable should be considered numberical\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['relevant'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-239d0fe08f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relevant'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mcont_rel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas-1.0.3-py3.7-macosx-10.14-x86_64.egg/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2804\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas-1.0.3-py3.7-macosx-10.14-x86_64.egg/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m         )\n\u001b[1;32m   1555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas-1.0.3-py3.7-macosx-10.14-x86_64.egg/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"loc\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1645\u001b[0m                 \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1648\u001b[0m             \u001b[0;31m# we skip the warning on Categorical/Interval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['relevant'] not in index\""
     ]
    }
   ],
   "source": [
    "#deviding the columns in the appropriate type\n",
    "cat = df.loc[:, df.nunique() < 30]\n",
    "cont = df.loc[:, df.nunique() >= 30]\n",
    "\n",
    "lst = cont.columns.tolist()\n",
    "lst.append('relevant')\n",
    "cont_rel = df[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We explore how the occurence of a certain categorical value increases the chanse of \"relevant\" to be true\n",
    "fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6), (ax7, ax8)) = plt.subplots(ncols=2, nrows=4,figsize=(17, 20))\n",
    "pd.crosstab(df.quality, df.relevant).plot(kind='bar', ax=ax1)\n",
    "pd.crosstab(df.intra_parts, df.relevant).plot(kind='bar', ax=ax2)\n",
    "pd.crosstab(df.skip_parts, df.relevant).plot(kind='bar', ax=ax3)\n",
    "pd.crosstab(df.inter_16x16_parts, df.relevant).plot(kind='bar', ax=ax4)\n",
    "pd.crosstab(df.inter_4x4_parts, df.relevant).plot(kind='bar', ax=ax5)\n",
    "pd.crosstab(df.inter_other_parts, df.relevant).plot(kind='bar', ax=ax6)\n",
    "pd.crosstab(df.frame_width, df.relevant).plot(kind='bar', ax=ax7)\n",
    "pd.crosstab(df.frame_height, df.relevant).plot(kind='bar', ax=ax8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check hoe the categorical variable correlate with the image relevantness to Colonoscopy\n",
    "plt.figure(figsize=(13, 9))\n",
    "corrMatrix = cat.corr()\n",
    "viz = sns.heatmap(corrMatrix, annot=True)\n",
    "\n",
    "viz.set_xticklabels(viz.get_xticklabels(), rotation=45)\n",
    "viz.set_yticklabels(viz.get_yticklabels(), rotation=45)\n",
    "\n",
    "#plt.savefig('CatCorr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we explore whether the means of variables subgroups being relevant of unrelevant to Colonoscope have a significantly different means\n",
    "cont_rel.groupby('relevant').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualize the mean difference of variables and cannot claim that any of variable subgroups are equal\n",
    "\n",
    "fig, ((ax1, ax2, ax3), (ax4, ax5, ax6), (ax7, ax8, ax9), (ax10, ax11, ax12), (ax13, ax14, ax15), (ax16, ax17, ax18)) = plt.subplots(ncols=3, nrows=6,figsize=(17, 20))\n",
    "sns.boxplot(y=cont_rel.columns[0], x=\"relevant\", data=df, ax=ax1)\n",
    "sns.boxplot(y=cont_rel.columns[1],x=\"relevant\", data=df, ax=ax2)\n",
    "sns.boxplot(y=cont_rel.columns[2], x=\"relevant\", data=df, ax=ax3)\n",
    "sns.boxplot(y=cont_rel.columns[3], x=\"relevant\", data=df, ax=ax4)\n",
    "sns.boxplot(y=cont_rel.columns[4], x=\"relevant\", data=df, ax=ax5)\n",
    "sns.boxplot(y=cont_rel.columns[5], x=\"relevant\", data=df, ax=ax6)\n",
    "sns.boxplot(y=cont_rel.columns[6], x=\"relevant\", data=df, ax=ax7)\n",
    "sns.boxplot(y=cont_rel.columns[7], x=\"relevant\", data=df, ax=ax8)\n",
    "sns.boxplot(y=cont_rel.columns[8], x=\"relevant\", data=df, ax=ax9)\n",
    "sns.boxplot(y=cont_rel.columns[9], x=\"relevant\", data=df, ax=ax10)\n",
    "sns.boxplot(y=cont_rel.columns[10], x=\"relevant\", data=df, ax=ax11)\n",
    "sns.boxplot(y=cont_rel.columns[11], x=\"relevant\", data=df, ax=ax12)\n",
    "sns.boxplot(y=cont_rel.columns[12], x=\"relevant\", data=df, ax=ax13)\n",
    "sns.boxplot(y=cont_rel.columns[13], x=\"relevant\", data=df, ax=ax14)\n",
    "sns.boxplot(y=cont_rel.columns[14], x=\"relevant\", data=df, ax=ax15)\n",
    "sns.boxplot(y=cont_rel.columns[15], x=\"relevant\", data=df, ax=ax16)\n",
    "sns.boxplot(y=cont_rel.columns[16], x=\"relevant\", data=df, ax=ax17)\n",
    "sns.boxplot(y=cont_rel.columns[17], x=\"relevant\", data=df, ax=ax18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "import random\n",
    "\n",
    "def equality_testing(df, variables, y):\n",
    "    \n",
    "    def check_normality(var):\n",
    "            normality = jarque_bera(var)\n",
    "            if float(normality[1]) < 0.5:\n",
    "                print(\" violates the normality!\")\n",
    "    \n",
    "\n",
    "    for el in variables:\n",
    "        zero_y = df.loc[df[y] == 0][el].tolist()\n",
    "        one_y = df.loc[df[y] == 1][el].tolist()\n",
    "        print(el.upper())\n",
    "        sample_size = max(len(zero_y), len(one_y))\n",
    "        zero_y = random.choices(zero_y, k = sample_size)\n",
    "        one_y = random.choices(one_y, k = sample_size)\n",
    "        check_normality(zero_y)\n",
    "        check_normality(one_y)\n",
    "        \n",
    "        if ttest_rel(zero_y, one_y).pvalue >=0.5:\n",
    "            print(\"!!!The groups related to 1 or 0 have the same mean :\" + el) \n",
    "        else:\n",
    "            print(\"!!!The groups related to 1 or 0 are different :\" + el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here with paired t-test (even the normality assumptions are not met) we claim that each of the subgroups are \n",
    "# statistically different and we cannot suspect any variable to be inneficient in identifying relevantnes to Colonoscopy \n",
    "equality_testing(cont_rel, cont_rel.columns[:-1].tolist(), cont_rel.columns[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we explore the correlation where we see the correlation rising from 0.02 to 0.25\n",
    "plt.figure(figsize=(18, 15))\n",
    "corrMatrix = cont_rel.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable selection and Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we generate new variables from old ones to reduce the cross-correlation in between the predictor variables and to decrese  needles the model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proportion of useful infomation per square\n",
    "df[\"pixels_height_width\"] = (df['frame_height']*df['frame_width'])/df['non_zero_pixels']\n",
    "df = df.drop(['frame_height', 'frame_width', 'non_zero_pixels'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted variance of sub-blocks\n",
    "df['sub_mean'] = (df['sub_mean_1']+ df['sub_mean_2'] + df['sub_mean_3'] +df['sub_mean_4'])/4\n",
    "df = df.drop(['sub_mean_1', 'sub_mean_2', 'sub_mean_3', 'sub_mean_4'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variability per block movement\n",
    "#df['movement'] = (df['var_movement_h'] + df['var_movement_v'])/(df['block_movement_h'] + df['block_movement_v'])\n",
    "df['movement_var'] = ((df['block_movement_h']/df['var_movement_h'])+(df['block_movement_v']/df['var_movement_v']))/2\n",
    "df = df.drop(['block_movement_h', 'block_movement_v', 'var_movement_h', 'var_movement_v'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average cost of the block encoding\n",
    "df['cost'] = (df['cost_1']+df['cost_2'])/2\n",
    "df = df.drop(['cost_1', 'cost_2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the mean of pixels encoded after Sobel operation\n",
    "df['sobel'] = (df['sobel_h']+df['sobel_v'])/2\n",
    "df = df.drop(['sobel_h', 'sobel_v'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we explore how the transformed variables correlate with the relevan and check if there is not much variability is shared among predictor variables\n",
    "plt.figure(figsize=(18, 15))\n",
    "corrMatrix = df.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting based on correlation pattern with other variables\n",
    "df = df.drop([\"var_sub_blocks\", \"sub_mean\", \"movement_var\", \"inter_16x16_parts\", \"mean\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation of the final correlation matrix of variables we use for predicting \"relevant\"\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "corrMatrix = df.corr()\n",
    "viz = sns.heatmap(corrMatrix, annot=True)\n",
    "viz.set_xticklabels(viz.get_xticklabels(), rotation=45)\n",
    "viz.set_yticklabels(viz.get_yticklabels(), rotation=45)\n",
    "plt.savefig('FinalCorr.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we see that number of observations to be releavant is 6 times higher than number or irrelevant observations\n",
    "%matplotlib inline\n",
    "sns.countplot(x='relevant', data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cheking the generated continuous data \n",
    "cont = df.loc[:, df.nunique() >= 30]\n",
    "lst = cont.columns.tolist()\n",
    "lst.append('relevant')\n",
    "cont_rel = df[lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we check how the generated and selected variables correlate with the relevance to colonoscopy\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, nrows=1,figsize=(17, 5))\n",
    "sns.boxplot(y=\"pixels_height_width\", x=\"relevant\", data=df, ax=ax1)\n",
    "sns.boxplot(y=\"cost\", x=\"relevant\", data=df, ax=ax2)\n",
    "sns.boxplot(y=\"sobel\", x=\"relevant\", data=df, ax=ax3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if all the sub-groups (relevant=0 and relevant=1) of variables are statistically different from each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All sub-groups of variables are statistically different from mean\n",
    "equality_testing(cont_rel, [\"pixels_height_width\", \"cost\", \"sobel\"], cont_rel.columns[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Trasformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The column that has less than 30 different values is considered to have categorical data\n",
    "cat = df.loc[:, df.nunique() < 30]\n",
    "cont = df.loc[:, df.nunique() >= 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization of continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cont.head())\n",
    "for var in cont.columns:\n",
    "    cont[var] = cont[var].apply(lambda x:  x / df[var].max())\n",
    "cont.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical data transofmations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We belive that the order for the selected categorical variables matters so we use Label encoder. \n",
    "# We found no need in One-Hot encoding as NONE of the variables had only categorycal difference (difference by class) and not by order.\n",
    "\n",
    "print(cat.head())\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# This technique gives the highest priority due to its label and lowest priority for its label being 0.\n",
    "encode = LabelEncoder()\n",
    "for el in cat.columns:\n",
    "    encode.fit(cat[el])\n",
    "    cat[el] = encode.transform(cat[el])\n",
    "cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([cont, cat], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As there are 6 times more relevant data then irrelevant from relevant we randomly select a sample of the size equal to the irrelevant data\n",
    "#(This will help to avoid the low recall)\n",
    "\n",
    "sample_size = df['relevant'].value_counts()[0]\n",
    "relevant = df[df.relevant == 0]\n",
    "not_relevant = df[df.relevant == 1].sample(sample_size , replace = False)\n",
    "relevant = relevant.reset_index(drop = True)\n",
    "not_relevant = not_relevant.reset_index(drop = True)\n",
    "fin_df = not_relevant.append(relevant)\n",
    "fin_df = shuffle(fin_df).reset_index(drop = True)\n",
    "fin_df = fin_df.dropna(axis=1)\n",
    "fin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving to the file\n",
    "fin_df.to_csv(\"tryMe.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing all the libraries for modelling \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn import tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deviding in explanatory variables and our target variable\n",
    "\n",
    "X = df[df.columns[:-1]]\n",
    "y = df[df.columns[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression model\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "logreg = LogisticRegression(max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "#print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(logreg.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "confusion = metrics.confusion_matrix(y_test,y_pred)\n",
    "\n",
    "TP = confusion[1,1]\n",
    "TN = confusion[0,0]\n",
    "FP = confusion[0,1]\n",
    "FN = confusion[1,0]\n",
    "\n",
    "accuracy = (TP + TN) / float(TP+TN+FP+FN) # metrics.accuracy_score(y_test, y_pred)\n",
    "sensitiviy = TP / float(TP+FN)  #recall metrics.recall_score(y_test, y_pred)\n",
    "specificity = TN / float(TN+FP) #when the actual value is negative, how often is the predicion correct?\n",
    "precision = TP / float(TP+FP)   #metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"METRICS FOR LOGISTIC REGRESSION\")\n",
    "print(\"accuracy\", accuracy.round(4))  \n",
    "print(\"recall\", sensitiviy.round(4))\n",
    "print(\"specificity\", specificity.round(4))\n",
    "print(\"precision\",precision.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree model\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train2, y_train2)\n",
    "\n",
    "y_pred2 = clf.predict(X_test2)\n",
    "\n",
    "\n",
    "confusion = metrics.confusion_matrix(y_test2,y_pred2)\n",
    "\n",
    "TP = confusion[1,1]\n",
    "TN = confusion[0,0]\n",
    "FP = confusion[0,1]\n",
    "FN = confusion[1,0]\n",
    "\n",
    "accuracy = (TP + TN) / float(TP+TN+FP+FN) # metrics.accuracy_score(y_test, y_pred)\n",
    "sensitiviy = TP / float(TP+TN)  #recall metrics.recall_score(y_test, y_pred)\n",
    "specificity = TN / float(TN+FP) #when the actual value is negative, how often is the predicion correct?\n",
    "precision = TP / float(TP+FP)   #metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "print(\"METRICS FOR DECISION TREE\")\n",
    "print(\"accuracy\", accuracy.round(4))  \n",
    "print(\"recall\", sensitiviy.round(4))\n",
    "print(\"specificity\", specificity.round(4))\n",
    "print(\"precision\",precision.round(4))\n",
    "\n",
    "\n",
    "#tree.plot_tree(clf) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest model\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier()\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(X_train3,y_train3)\n",
    "\n",
    "y_pred3=clf.predict(X_test3)\n",
    "\n",
    "confusion = metrics.confusion_matrix(y_test3,y_pred3)\n",
    "\n",
    "TP = confusion[1,1]\n",
    "TN = confusion[0,0]\n",
    "FP = confusion[0,1]\n",
    "FN = confusion[1,0]\n",
    "\n",
    "accuracy = (TP + TN) / float(TP+TN+FP+FN) # metrics.accuracy_score(y_test, y_pred)\n",
    "sensitiviy = TP / float(TP+TN)  #recall metrics.recall_score(y_test, y_pred)\n",
    "specificity = TN / float(TN+FP) #when the actual value is negative, how often is the predicion correct?\n",
    "precision = TP / float(TP+FP)   #metrics.precision_score(y_test, y_pred)\n",
    "\n",
    "print(\"METRICS FOR RANDOM FOREST\")\n",
    "print(\"accuracy\", accuracy.round(4))  \n",
    "print(\"recall\", sensitiviy.round(4))\n",
    "print(\"specificity\", specificity.round(4))\n",
    "print(\"precision\",precision.round(4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
